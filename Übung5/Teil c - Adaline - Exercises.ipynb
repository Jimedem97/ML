{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil c - Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptives Lineares Neuron (Adaline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weiterentwicklung des Perzeptrons\n",
    "\n",
    "Das Perzeptron liefert keine stabile L√∂sung, falls das Problem nicht linear trennbar ist. Stabil bedeutet, wenn ein neues Trainingsbeispiel betrachtet wird, nicht sofort alles vergisst, was bis zu diesem Zeitpunkt gelernt wurde. Aus diesem Grund haben Widrow und Hoff ein neuronales Netz vorgeschlagen, das genau diese Eigenschaften besitzt. Sie nannten es *adaptive linear* (ADAptive LInear Neuron) bzw. Adaline. Adaline ist ein weiteres neuronales Netz mit einer einzigen Schicht. Adaline wurde nur wenige Jahre nach dem Perzeptron-Algorithmus ver√∂ffentlicht und kann als Verbesserung aufgefasst werden. Es bildet die Grundlage f√ºr fortgeschrittene Lernalgorithmen f√ºr die Klassifizierung, wie bspw. die logistische Regression, Support Vector Machines, Multi-layer Perceptrons, etc.\n",
    "\n",
    "### √Ñnderungen der Gewichtungen\n",
    "Die Berechnung der √Ñnderungen der Gewichte findet bei beiden Algorithmen unterschiedlich statt: <br>\n",
    "\n",
    "<b>Perzeptron</b>: <br>\n",
    "\n",
    "Beim Perzeptron wird der Ouput, dh die Klassifikation des Inputs, zur Fehlerkorrektur verwendet. Das bedeutet, dass ein Bin√§rwert f√ºr die Fehlerkorrektur verwendet wird. <br>\n",
    "\n",
    "$\\Delta w_j = \\eta \\cdot (y^{(i)} - \\hat{y}^{(i)}) \\cdot x_j^{(i)}$ <br>\n",
    "\n",
    "<b>Adaline</b>: <br>\n",
    "\n",
    "\n",
    "Beim Adaline wird der Net Input (dh die gewichtete summe $s$ bzw. $net$) zur Fehlerkorrektur verwendet. Das bedeutet, dass ein kontinuierlicher Wert f√ºr die Fehlerkorrektur verwendet wird. Das sorgt daf√ºr, dass die √Ñnderungen an den Gewichten besser in Relation zu den Fehlern stehen. Ein weiterer Unterschied beim Adaline-Algorithmus ist, dass die Berechnung der Gewichtsaktualisierung auf allen Trainingsobjekten erfolgt. Aus diesem Grund wird diese Form des Lernalgorithmus als Stapelverarbeitung bezeichnet.\n",
    "\n",
    "\n",
    "$\\Delta w_j = \\eta \\cdot (y - net) \\cdot x_j$ <br>\n",
    "\n",
    "### Lernen im Adaline-Algorithmus\n",
    "\n",
    "Der wesentliche Unterschied des Adaline-Algorithmus besteht darin, dass die Aktualisierung der Gewichtungen auf einer linearen Aktivierungsfunktion beruht. Allerdings ist $\\phi(s)$ mit $s$ = $\\vec{w}^T \\vec{x} +w_0$ eine reele Zahl und keine ganzzahlige Klassenbezeichnung.  Bei Adaline ist diese Funktion $\\phi(s)$ einfach die identische Abbildung der Nettoeingabefunktion, sodass $\\phi(s) = \\vec{w}^T \\vec{x}+w_0$. Die lineare Aktivierungsfunktion wird dazu genutzt die Gewichtungen zu lernen. Anschlie√üend kann eine Schwellenwertfunktion (besitzt √Ñhnlichkeit mit der bereits bekannten Sprungfunktion) verwendet werden, um die Klassenbezeichnungen vorherzusagen. <br>\n",
    "\n",
    "Bei fortgeschrittenen Lernalgorithmen, wie bspw. dem Multi-layer Perceptron, kann als Aktivierungsfunktion die Sigmoid-Funktion ( [Siehe Web-Link](https://en.wikipedia.org/wiki/Sigmoid_function) ) eingesetzt werden.\n",
    "\n",
    "Folgende Abbildung zeigt, dass der Adaline-Algorithmus die tats√§chlichen Klassenzeichnungen mit den stetigen Ausgaben der linearen Aktivierungsfunktion vergleicht. Um Fehler des Modells zu berechnen und die Gewichtungen zu aktualisieren. <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Figures/Adaline.png\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "Zum Vergleich, das Perzeptron aktualisiert die Gewichtungen nach jedem Objekt inkrementell. Des Weiteren beruht beim Perzeptron die Aktualisierung der Gewichtungen auf einer einfachen Sprungfunktion. Das Perzeptron vergleicht die tats√§chlichen Klassenzeichnungen mit den vorhergesagten Klassenzeichnungen.\n",
    "\n",
    "<img src=\"./Figures/Perzeptron.png\" alt=\"drawing\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung\n",
    "\n",
    "Die Implementierungen der Adaline-Regel und der Perzeptron-Regel sind sich sehr √§hnlich. Die Implementierung erfolgt innerhalb der Klasse <b>Adaline</b>. Im folgenden werden die einzelnen Methoden und deren Funktionsweise kurz vorgestellt. <br>\n",
    "\n",
    "#### Konstruktor\n",
    "Das Perceptron-Objekt wird mit der Lernrate <b>eta</b> und der Anzahl der Epochen (Durchl√§ufe der Trainingsdaten) <b>epochs</b> initialisiert. W√§hlen Sie geeignete Werte f√ºr die Epoche (Anzahl der Durchl√§ufe) und die Lernrate Eta.\n",
    "\n",
    "#### activation()-Methode:\n",
    "Diese Funktion bewirkt im Code nichts, denn es handelt sich um eine Identit√§tsfunktion. Sie existiert zur Demonstration, wie Informationen durch ein einschichtiges Neuronales Netz flie√üen: Merkmale der Eingabedaten, Nettoeingabe, Aktivierungsfunktion und Ausgabe. <br>\n",
    "\n",
    "Weitere Klassifizierer (wie bspw. logistische Regression, Multi-layer Perzeptron, etc.) sind sehr eng mit Adaline verwandt. Denn sie unterscheiden sich in der Aktivierungs- und Straffunktion.\n",
    "\n",
    "#### fit()-Methode:\n",
    "\n",
    "<b>Gewichtungen</b>: <br>\n",
    "Die Gewichtungen werden wie beim Perzeptron initialisiert. <br>\n",
    "Die Aktualisierung der Gewichtung der Bias-Einheit wird anhand der Summe der Errors berechnet.<br>\n",
    "Die Aktualisierung der weiteren Gewichtungen von 1 bis m wird anhand <b>X.T.dot(errors)</b> berechnet. Hierbei handelt es sich um eine *Matrix-Vektor-Multiplikation* von Merkmalsmatrix und Fehlervektor. <br>\n",
    "\n",
    "Die Berechnung der Gewichtsaktualisierung erfolgt auf allen Trainingsobjekten. Zum Vergleich, beim Perzeptron werden die Gewichtungen nach der Berechnung jedes einzelnen Trainingsexemplar aktualisiert. <br>\n",
    "\n",
    "Sammeln Sie in einer Liste <b>cost</b> die in jeder Epoche auftretenden quadrierten Fehler nach folgender Formel: <b>cost_epoch= (error**2).sum() / 2.0</b>. Dadurch kann sp√§ter analysiert werden, wie gut der Adline-Algorithmus w√§hrend des Trainings funktioniert hat. Geben Sie diese Liste als R√ºckgabewert der Methode zur√ºck.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Œîùë§ùëó=ùúÇ‚ãÖ(ùë¶‚àíùëõùëíùë°)‚ãÖùë•ùëó "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline(object):\n",
    "    \n",
    "    def __init__(self, eta=None, epochs=None):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def gewichtete_summe(self, X):\n",
    "        # TODO: implement\n",
    "        return None \n",
    "    \n",
    "    def activation(self, X):\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        errors = []\n",
    "        self.weights = np.random.normal(size=(X.shape[0] + 1))\n",
    "        self.weights[0] = 1\n",
    "        for xi, target in zip(X, y):\n",
    "            \n",
    "        return None \n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, eta=0.001, epochs=30):\n",
    "        \n",
    "        \n",
    "    def gewichtete_summe(self, x):\n",
    "        summe = np.dot(np.transpose(self.weights[1:]), x) + self.weights[0]\n",
    "        return summe \n",
    "        \n",
    "    def heaviside(self, summe):\n",
    "        if summe < 0:\n",
    "          return 0\n",
    "        else:\n",
    "          return 1\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "       \n",
    "        for i in range(self.epochs):\n",
    "            error = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.heaviside(self.gewichtete_summe(X[xi])))\n",
    "                self.weights[1:] =  self.weights[1:] + (update * X[xi])\n",
    "                self.weights[0] += update\n",
    "                error += int(update != 0)\n",
    "            errors.append(error)\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz, Training und Visualisierung des Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W√§hlen Sie denselben Datensatz wie beim Perzeptron. <br>\n",
    "F√ºhren Sie das Training anhand verschiedener Parameter aus und visualisieren Sie die Ergebnisse. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
